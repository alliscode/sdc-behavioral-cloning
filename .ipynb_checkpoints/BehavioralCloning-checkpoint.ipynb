{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Behavioral cloning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile model.py\n",
    "\n",
    "\"\"\"This module can be used to build and train a convolutional nueral network used to predict\n",
    "an appropriate steering angle for a self driving car based on images from a center mounted\n",
    "dash cam.\n",
    "\n",
    "Example:\n",
    "    \n",
    "    !) Build a model based on the VGG16 CNN\n",
    "    2) Train it for 5 epochs\n",
    "    3) Use it to make a prediction\n",
    "    \n",
    "    recorded_data = ... # array trianing data paths\n",
    "    \n",
    "    batch_size = 128\n",
    "    trainGen, trainSize, valGen, valSize = getGenerators(recorded_data, batch_size)\n",
    "    trainGenerator, validationGenerator = ...\n",
    "    \n",
    "    driver = Driver((224, 224, 3))\n",
    "    driver.build(ModelType.VGG16)\n",
    "    driver.trainGen(trainGen, trainSize, 5, valGen, valSize)\n",
    "    \n",
    "    driver.predict\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "__author__  =  \"Ben Thomas\"\n",
    "\n",
    "import os.path\n",
    "import numpy as np\n",
    "from enum import Enum\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Activation, Flatten, Dropout\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model, load_model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "class ModelType(Enum):\n",
    "    \"\"\"An enum type for specifying a model architexture.\"\"\"\n",
    "    \n",
    "    CONV1 = 1,\n",
    "    VGG16 = 2\n",
    "\n",
    "class Driver:\n",
    "    \"\"\"\"\"\"\n",
    "    \n",
    "    def __init__(self, input_shape):\n",
    "        \"\"\"Constructs an instance of the Driver class.\n",
    "        \n",
    "        Args:\n",
    "            input_shape (int, int, int): The shape of the images that will be used with the model.\n",
    "            \n",
    "        Returns:\n",
    "            Driver: The initialized instance.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.input_shape = input_shape\n",
    "        \n",
    "    def build(self, model_type):\n",
    "        \"\"\"Builds a convolutional neural network.\n",
    "        \n",
    "        Args:\n",
    "            model_type (ModelType): The type of model to build.\n",
    "        \"\"\"\n",
    "        \n",
    "        if model_type == ModelType.CONV1:\n",
    "            self.model = self.basicModel()\n",
    "        elif model_type == ModelType.VGG16:\n",
    "            self.model = self.vgg16()\n",
    "            \n",
    "    def load(self, model_path):\n",
    "        \"\"\"Loads the model at the provided path.\n",
    "        \n",
    "        Args:\n",
    "            model_path: A path to a valid Keras model (.h5)\n",
    "        \"\"\"\n",
    "        \n",
    "        self.model = load_model(model_path)\n",
    "        \n",
    "    def __str__(self):\n",
    "        \"\"\"Provides a summary of the model.\"\"\"\n",
    "        \n",
    "        if self.model is None:\n",
    "            return \"Model is not built.\"\n",
    "        \n",
    "        self.model.summary()\n",
    "        return \"\"\n",
    "        \n",
    "    def basicModel(self):\n",
    "        \"\"\"A 'basic' convolutional neural network. This network contains 3 convolutional blocks\n",
    "        and 2 fully connected layers followed by a dropout layer before the output neuron.\n",
    "        \n",
    "        Returns:\n",
    "            keras.models.Model: The constructed model. The model returned from this \n",
    "            function has not yet been compiled.\n",
    "        \"\"\"\n",
    "        \n",
    "        # this model will be pretty basic with a single input, single output and no branching.\n",
    "        # Because of this, the keras Sequential model will work fine\n",
    "        model = Sequential()\n",
    "        \n",
    "        # convolution 1, max-pooling, relu for the nonlinearity\n",
    "        model.add(Convolution2D(50, 15, 15, border_mode='valid', input_shape=self.input_shape))\n",
    "        model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "        model.add(Activation('relu'))\n",
    "        \n",
    "        # convolution 2, max-pooling, relu for the nonlinearity\n",
    "        model.add(Convolution2D(30, 10, 10, border_mode='valid'))\n",
    "        model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "        model.add(Activation('relu'))\n",
    "        \n",
    "        # convolution 3, max-pooling, relu for the nonlinearity\n",
    "        model.add(Convolution2D(10, 7, 7, border_mode='valid'))\n",
    "        model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "        model.add(Activation('relu'))\n",
    "        \n",
    "        # flatten the filters before transitioning to the fully connected section\n",
    "        model.add(Flatten())\n",
    "        \n",
    "        # fully connected 1 with 200 neurons, relu for the nonlinearity\n",
    "        model.add(Dense(200))\n",
    "        model.add(Activation('relu'))\n",
    "        \n",
    "        # fully connected 2 with 75 neurons, relu for the nonlinearity\n",
    "        model.add(Dense(75))\n",
    "        model.add(Activation('relu'))\n",
    "        \n",
    "        # dropout to help prevent over-fitting to the training data\n",
    "        model.add(Dropout(0.25))\n",
    "        \n",
    "        # as currently stated, this is a regression problem. Because of this, the output layer is a single \n",
    "        # neuron with a continuous value and so softmax activations make no sense. Linear makes the most\n",
    "        # sense in this case.\n",
    "        model.add(Dense(1))\n",
    "        model.add(Activation('linear'))\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def vgg16(self):\n",
    "        \"\"\"A more complex convolutional neural network. This network is based on the VGG16 CNN provided\n",
    "        by Keras (https://keras.io/applications/). The pre-trained VGG16 model is followed up with a custom \n",
    "        top section to perform the regression task. The pre-trained VGG16 layers are frozen so training this\n",
    "        model will only effect the top section.\n",
    "        \n",
    "        Returns:\n",
    "            keras.models.Model: The constructed model. The model returned from this \n",
    "            function has not yet been compiled.\n",
    "        \"\"\"\n",
    "        \n",
    "        # The model architecture here is too complicated to use the Sequential model provided by Keras\n",
    "        # so the functional API is used instead (https://keras.io/getting-started/functional-api-guide/).\n",
    "        # This makes it easy to add a regression section on top of the pre-trained VGG16 model.\n",
    "        input_layer = Input(shape=self.input_shape)\n",
    "        \n",
    "        # Create the VGG16 model. We don't include the top (fully connected layers) because that would\n",
    "        # lock us in to using images with a shape of 224x224x3 as was done in the original VGG16 training.\n",
    "        # We will provide our own fully connected section. (https://keras.io/applications/#vgg16)\n",
    "        vgg16_trained = VGG16(weights='imagenet', include_top=False, input_tensor=input_layer)\n",
    "        \n",
    "        # we want the freeze all of the layers of VGG16 to take adavantage of the pre-trained weights. \n",
    "        # Then we can train our custom top section to perform the regression task.\n",
    "        for layer in vgg16_trained.layers:\n",
    "            layer.trainable = False\n",
    "        \n",
    "        # build the model using Keras functional API. The output of the VGG16 convolutional sections are \n",
    "        # flattened before transitioning to the fully connected layers. Relu activations are used at each\n",
    "        # step to introduce nonlinearity. Furthermore, dropout is added at several points to help prevent\n",
    "        # this very large model from over-fitting our relatively small dataset.\n",
    "        model = vgg16_trained.output\n",
    "        model = Flatten()(model)\n",
    "        model = Dense((1500))(model)\n",
    "        model = Dropout(0.5)(model)\n",
    "        model = Activation('relu')(model)\n",
    "        model = Dropout(0.5)(model)\n",
    "        model = Dense((750))(model)\n",
    "        model = Activation('relu')(model)\n",
    "        model = Dense(1)(model)\n",
    "        model = Activation('linear')(model)\n",
    "        return Model(input=input_layer, output=model)\n",
    "    \n",
    "    def train(self, X, y, batch_size=128, nb_epoch=5, lr=0.001):\n",
    "        \"\"\"Train the model with provided samples and labels. The training is performed using the Adam\n",
    "        optimizer and the mean-squared-error loss function.\n",
    "        \n",
    "        Args:\n",
    "            X: The training smamples\n",
    "            y: The training labels\n",
    "            batch_size (optional): the size of training batches. Defaults to 128\n",
    "            nb_epoch (optional): the number of epochs to train for. Defaults to 5\n",
    "            lr (optional): The learning rate used in the Adam optimizer. Defaults to 0.001\n",
    "        \"\"\"\n",
    "        \n",
    "        optimizer = Adam(lr=lr)\n",
    "        self.model.compile(optimizer, loss='mse')\n",
    "        history = self.model.fit(X, y, batch_size, nb_epoch)\n",
    "    \n",
    "    def trainGen(self, train_generator, sample_per_epoch, nb_epoch, validation_generator, validation_size, lr=0.001):\n",
    "        \"\"\"Train the model with provided data generators. The training is performed using the Adam\n",
    "        optimizer and the mean-squared-error loss function.\n",
    "        \n",
    "        Args:\n",
    "            train_generator: A generator that yields a batch of training images and labels\n",
    "            sample_per_epoch: The number of samples/labels in each epoch\n",
    "            nb_epoch: The number of epochs to train for. Defaults to 5\n",
    "            validation_generator: A generator that yields a batch of validation images and labels\n",
    "            validation_size: The number of samples used in the validation step.\n",
    "            lr (optional): The learning rate used in the Adam optimizer. Defaults to 0.001\n",
    "        \"\"\"\n",
    "        \n",
    "        optimizer = Adam(lr=lr)\n",
    "        self.model.compile(optimizer, loss='mse')\n",
    "        history = self.model.fit_generator(train_generator, sample_per_epoch, nb_epoch, validation_data=validation_generator, nb_val_samples=validation_size)\n",
    "\n",
    "    def predict(self, image):\n",
    "        \"\"\"Uses the already trained model to predict the appropriate steering angle for a given input image.\n",
    "        Args:\n",
    "            image: The image that the prediction is based on. The dimensions of the image must match the\n",
    "            dimensions that the network was trained on.\n",
    "            \n",
    "        Returns (float) The predicted steering angle.\n",
    "        \"\"\"\n",
    "        return float(self.model.predict(image, batch_size=1))\n",
    "    \n",
    "    def save(self, file):\n",
    "        \"\"\"Saves the trained model to disk.\n",
    "        \n",
    "        Args:\n",
    "            file (string): The name of the samed file. An extension of 'h5' will be appended to the file name if\n",
    "            it is not already.\n",
    "        \"\"\"\n",
    "        \n",
    "        if os.path.splitext(file)[1] != 'h5':\n",
    "            file = file + '.h5'\n",
    "            \n",
    "        self.model.save(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting process.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile process.py\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def read_image(image_path):\n",
    "    \"\"\"Reads an image from disk into a numpy array.\n",
    "    \n",
    "    Args:\n",
    "        image_path (string): The path of the image to be read.\n",
    "        \n",
    "    Returns: (numpy.Array): A numpy array containing the image data.\n",
    "    \"\"\"\n",
    "    \n",
    "    image = cv2.imread(image_path)\n",
    "    return image\n",
    "\n",
    "def pre_process(image):\n",
    "    \"\"\"Preprocess an image for use in the CNN.\n",
    "    \n",
    "    Args:\n",
    "        image (numpy.array): The image to process.\n",
    "    \n",
    "    Returns: (numpy.Array): The processed image.\n",
    "    \"\"\"\n",
    "    \n",
    "    # discard the top of the image\n",
    "    height = int(0.6 * image.shape[0])\n",
    "    image = image[-height:-1,:,:]\n",
    "    \n",
    "    #resize to 80x80\n",
    "    image = cv2.resize(image, (80, 80))\n",
    "        \n",
    "    # convert to gray scale floats\n",
    "    shape = image.shape\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY).reshape(shape[0], shape[1], 1).astype(np.float32)\n",
    "    \n",
    "    # scale the image\n",
    "    image = image/255.0 - 0.5\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile data.py\n",
    "\n",
    "from process import read_image, pre_process\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "import os.path\n",
    "import cv2\n",
    "\n",
    "class TrainingData:\n",
    "    \"\"\"A class to represent and manage the training data for the CNN.\n",
    "        \n",
    "    Args: \n",
    "        training_sets: An list of paths to training sets. Each path must point to a directory\n",
    "        that contains a driving_log.csv file as well as a directory of trianing images.\n",
    "        batch_size (optional): The batch size that will be yielded by the generators. Defaults to 128\n",
    "        validation_split (optional): The portion of data to split out for validation.\n",
    "    \n",
    "    Attributes:\n",
    "        train_generator: A generator that yields a collection of training images and a collection\n",
    "        training labels.\n",
    "        validation_generator: A generator that yields a collection of valiation images and a collection\n",
    "        validation labels.\n",
    "        training_size: The number of training samples.\n",
    "        validation_size: The number of validation samples\n",
    "        training_sets: An list of paths to training sets. Each path must point to a directory\n",
    "        that contains a driving_log.csv file as well as a directory of trianing images.\n",
    "        batch_size (optional): The batch size that will be yielded by the generators. Defaults to 128\n",
    "        validation_split (optional): The portion of data to split out for validation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, training_sets, batch_size=128, validation_split=0.2):\n",
    "        self.training_sets = training_sets\n",
    "        self.batch_size = batch_size\n",
    "        self.validation_split = validation_split\n",
    "        self.training_generator = None\n",
    "        self.validation_generator = None\n",
    "        self.training_size = None\n",
    "        self.validation_size = None\n",
    "        self.load_data()\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"Loads the provide training sets by creating generators for the test and validation sets.\"\"\"\n",
    "        \n",
    "        self.sample_maps = []\n",
    "\n",
    "        # merge all of the sample maps\n",
    "        for directory in self.training_sets:\n",
    "            mapFile = directory + '/driving_log.csv'\n",
    "            images = directory + '/IMG'\n",
    "\n",
    "            # validate the input\n",
    "            if not os.path.isdir(images) or not os.path.isfile(mapFile):\n",
    "                raise ValueError(\"parameter must be a valid directory containing images and csv map.\")\n",
    "\n",
    "            # read the contents of the map file\n",
    "            with open(mapFile) as f:\n",
    "                self.sample_maps.extend(f.readlines())\n",
    "\n",
    "        all_samples = np.arange(0, len(self.sample_maps))\n",
    "        np.random.shuffle(all_samples)\n",
    "        validation_split_index = int(len(all_samples) * self.validation_split)\n",
    "        validation_samples, train_samples = all_samples[:validation_split_index], all_samples[validation_split_index:]\n",
    "        \n",
    "        self.training_generator = self.generator(train_samples)\n",
    "        self.validation_generator = self.generator(validation_samples)\n",
    "        self.validation_size = len(validation_samples)\n",
    "        self.training_size = len(train_samples)\n",
    "    \n",
    "    def generator(self, samples):\n",
    "        \"\"\"A generator used to feed data into the model fit routine.\n",
    "        \n",
    "        Args:\n",
    "            sample: A list of samples defining the image and labels to use.\n",
    "            \n",
    "        Yeilds: A tuple containing a collection of images and a collection of associated labels.\n",
    "        \"\"\"\n",
    "        \n",
    "        num_samples = len(samples)\n",
    "        while True:\n",
    "            batch_X = []\n",
    "            batch_y = []\n",
    "\n",
    "            # shuffle up the images\n",
    "            np.random.shuffle(samples)\n",
    "            for i, sample_index in np.ndenumerate(samples):\n",
    "                center, left, right, steering_angle, throttle, brake, speed = tuple(self.sample_maps[sample_index].split(','))\n",
    "                steering_angle, throttle, brake, speed = float(steering_angle), float(throttle), float(brake), float(speed)\n",
    "                if speed > 1.0:\n",
    "                    center_image = pre_process(read_image(center))\n",
    "                    batch_X.append(center_image)\n",
    "                    batch_y.append(steering_angle)\n",
    "\n",
    "                    if len(batch_X) % self.batch_size == 0 or i == (num_samples-1):\n",
    "                        yield (np.array(batch_X), np.array(batch_y))\n",
    "                        batch_X = []\n",
    "                        batch_y = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile train.py\n",
    "\n",
    "from model import Driver, ModelType\n",
    "from data import TrainingData\n",
    "\n",
    "\"\"\"The training of the CNN is performed in two steps. The first step runs with a limited training\n",
    "set for 5 epochs with a learning rate of 0.001. This has been found to establish a solid foundation\n",
    "that allows the car to make it all the way around the track, albeit in a rather slopy way. The second\n",
    "step trains with a lower learning rate and much more data for an additional 5 epochs. This fills in \n",
    "the gaps in the CNN and smooths out the driving considerably.\n",
    "\"\"\"\n",
    "\n",
    "# build the driver\n",
    "driver = Driver((80, 80, 1))\n",
    "driver.build(ModelType.CONV1)\n",
    "\n",
    "##################### Initial training ####################\n",
    "initial_data = [\n",
    "                    './data/trk1_normal_1', \n",
    "                    './data/trk1_normal_2', \n",
    "                    './data/trk1_normal_3', \n",
    "                    './data/trk1_corner_infill',\n",
    "                    './data/udacity_data',\n",
    "                ]\n",
    "\n",
    "# 1) The initial training step\n",
    "data = TrainingData(initial_data)\n",
    "driver.trainGen(data.training_generator, \n",
    "                data.training_size, \n",
    "                5, \n",
    "                data.validation_generator, \n",
    "                data.validation_size,\n",
    "                lr=0.001)\n",
    "\n",
    "####################### Fine tuning #######################\n",
    "fine_tune_data = [\n",
    "                    './data/trk1_normal_1', \n",
    "                    './data/trk1_normal_2', \n",
    "                    './data/trk1_normal_3', \n",
    "                    './data/trk1_normal_4', \n",
    "                    './data/trk1_swerve_fill', \n",
    "                    './data/trk1_corner_infill',\n",
    "                    './data/trk1_corner_infill_2',\n",
    "                    './data/trk1_bridge_infill',\n",
    "                    './data/trk1_corners',\n",
    "                    './data/trk2_normal_1',\n",
    "                    './data2/trk1_corrections_1',\n",
    "                    './data2/trk1_corrections_2',\n",
    "                    './data2/trk1_small_swerve',\n",
    "                    './data2/trk1_small_swerve_2',\n",
    "                    './data2/trk1_small_swerve_3',\n",
    "                    './data2/trk1_normal_1',\n",
    "                    './data/udacity_data',\n",
    "                ]\n",
    "\n",
    "# 2) The fine-tuning step\n",
    "data = TrainingData(fine_tune_data)\n",
    "driver.trainGen(data.training_generator, \n",
    "                data.training_size, \n",
    "                5, \n",
    "                data.validation_generator, \n",
    "                data.validation_size,\n",
    "                lr=0.0001)\n",
    "\n",
    "# write the model to disk\n",
    "driver.save('model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from model import Driver\n",
    "from keras.utils.visualize_util import plot\n",
    "\n",
    "# inspect the model\n",
    "\n",
    "driver = Driver((80,80,1))\n",
    "driver.load('./model.h5')\n",
    "print(driver)\n",
    "\n",
    "plot(driver.model, to_file='basic_model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_3 (InputLayer)             (None, 80, 80, 3)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "block1_conv1 (Convolution2D)     (None, 80, 80, 64)    1792        input_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "block1_conv2 (Convolution2D)     (None, 80, 80, 64)    36928       block1_conv1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block1_pool (MaxPooling2D)       (None, 40, 40, 64)    0           block1_conv2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block2_conv1 (Convolution2D)     (None, 40, 40, 128)   73856       block1_pool[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "block2_conv2 (Convolution2D)     (None, 40, 40, 128)   147584      block2_conv1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block2_pool (MaxPooling2D)       (None, 20, 20, 128)   0           block2_conv2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block3_conv1 (Convolution2D)     (None, 20, 20, 256)   295168      block2_pool[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "block3_conv2 (Convolution2D)     (None, 20, 20, 256)   590080      block3_conv1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block3_conv3 (Convolution2D)     (None, 20, 20, 256)   590080      block3_conv2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block3_pool (MaxPooling2D)       (None, 10, 10, 256)   0           block3_conv3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block4_conv1 (Convolution2D)     (None, 10, 10, 512)   1180160     block3_pool[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "block4_conv2 (Convolution2D)     (None, 10, 10, 512)   2359808     block4_conv1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block4_conv3 (Convolution2D)     (None, 10, 10, 512)   2359808     block4_conv2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block4_pool (MaxPooling2D)       (None, 5, 5, 512)     0           block4_conv3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block5_conv1 (Convolution2D)     (None, 5, 5, 512)     2359808     block4_pool[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "block5_conv2 (Convolution2D)     (None, 5, 5, 512)     2359808     block5_conv1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block5_conv3 (Convolution2D)     (None, 5, 5, 512)     2359808     block5_conv2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block5_pool (MaxPooling2D)       (None, 2, 2, 512)     0           block5_conv3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 2048)          0           block5_pool[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 1500)          3073500     flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 1500)          0           dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 1500)          0           dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 1500)          0           activation_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 750)           1125750     dropout_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_2 (Activation)        (None, 750)           0           dense_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 1)             751         activation_2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_3 (Activation)        (None, 1)             0           dense_3[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 18,914,689\n",
      "Trainable params: 4,200,001\n",
      "Non-trainable params: 14,714,688\n",
      "____________________________________________________________________________________________________\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from model import Driver, ModelType\n",
    "\n",
    "# inspect the model\n",
    "\n",
    "driver = Driver((80,80,3))\n",
    "driver.build(ModelType.VGG16)\n",
    "print(driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create some images for the report\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from process import read_image, pre_process\n",
    "import matplotlib.gridspec as gridspec\n",
    "import cv2\n",
    "\n",
    "data_dir = './data/trk1_normal_1'\n",
    "mapFile = data_dir + '/driving_log.csv'\n",
    "images = data_dir + '/IMG'\n",
    "data_points = []\n",
    "\n",
    "# read the contents of the map file\n",
    "with open(mapFile) as f:\n",
    "    data_points.extend(f.readlines())\n",
    "    \n",
    "fig = plt.figure(figsize=(10, 3))\n",
    "fig.suptitle('The preprocessing pipline', size=15)\n",
    "gs1 = gridspec.GridSpec(1, 2)\n",
    "\n",
    "np.random.seed(1050)\n",
    "index = int(np.random.random()*len(data_points))\n",
    "center, left, right, steering_angle, throttle, brake, speed = tuple(data_points[index].split(','))\n",
    "img = cv2.cvtColor(read_image(center), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "axes = plt.subplot(gs1[0])\n",
    "axes.set_xticklabels([])\n",
    "axes.set_yticklabels([])\n",
    "plt.imshow(img)\n",
    "plt.xlabel('Original image')\n",
    "\n",
    "axes = plt.subplot(gs1[1])\n",
    "axes.set_xticklabels([])\n",
    "axes.set_yticklabels([])\n",
    "plt.imshow(np.squeeze(pre_process(img)), cmap='gray')\n",
    "plt.xlabel('Preprocessed image')\n",
    "\n",
    "plt.show()\n",
    "fig.savefig('./preprocess.png', bbox_inches='tight')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
